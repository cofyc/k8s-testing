diff --git a/pkg/controller/volume/scheduling/BUILD b/pkg/controller/volume/scheduling/BUILD
index c9e2ace..a106e08 100644
--- a/pkg/controller/volume/scheduling/BUILD
+++ b/pkg/controller/volume/scheduling/BUILD
@@ -7,6 +7,7 @@ go_library(
         "scheduler_binder.go",
         "scheduler_binder_cache.go",
         "scheduler_binder_fake.go",
+        "scheduler_binder_plugin.go",
     ],
     importpath = "k8s.io/kubernetes/pkg/controller/volume/scheduling",
     visibility = ["//visibility:public"],
diff --git a/pkg/scheduler/algorithmprovider/defaults/register_predicates.go b/pkg/scheduler/algorithmprovider/defaults/register_predicates.go
index 7f16c65..05a40dc 100644
--- a/pkg/scheduler/algorithmprovider/defaults/register_predicates.go
+++ b/pkg/scheduler/algorithmprovider/defaults/register_predicates.go
@@ -121,8 +121,8 @@ func init() {
 	// Fit is determined by volume topology requirements.
 	factory.RegisterFitPredicateFactory(
 		predicates.CheckVolumeBindingPred,
-		func(args factory.PluginFactoryArgs) predicates.FitPredicate {
-			return predicates.NewVolumeBindingPredicate(args.VolumeBinder)
+		func(_ factory.PluginFactoryArgs) predicates.FitPredicate {
+			return predicates.NewVolumeBindingPredicate(nil) // replaced by volume binder plugin
 		},
 	)
 }
diff --git a/pkg/scheduler/api/compatibility/compatibility_test.go b/pkg/scheduler/api/compatibility/compatibility_test.go
index 0d3e062..e459407 100644
--- a/pkg/scheduler/api/compatibility/compatibility_test.go
+++ b/pkg/scheduler/api/compatibility/compatibility_test.go
@@ -553,7 +553,6 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 				"MaxAzureDiskVolumeCount",
 				"MatchInterPodAffinity",
 				"GeneralPredicates",
-				"CheckVolumeBinding",
 				"TestServiceAffinity",
 				"TestLabelsPresence",
 			),
@@ -571,6 +570,7 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 			),
 			wantFilterPlugins: sets.NewString(
 				"TaintToleration",
+				"VolumeBinder",
 			),
 			wantExtenders: []schedulerapi.ExtenderConfig{{
 				URLPrefix:        "/prefix",
@@ -652,7 +652,6 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 				"MaxAzureDiskVolumeCount",
 				"MatchInterPodAffinity",
 				"GeneralPredicates",
-				"CheckVolumeBinding",
 				"TestServiceAffinity",
 				"TestLabelsPresence",
 			),
@@ -670,6 +669,7 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 			),
 			wantFilterPlugins: sets.NewString(
 				"TaintToleration",
+				"VolumeBinder",
 			),
 			wantExtenders: []schedulerapi.ExtenderConfig{{
 				URLPrefix:        "/prefix",
@@ -763,7 +763,6 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 				"MaxAzureDiskVolumeCount",
 				"MatchInterPodAffinity",
 				"GeneralPredicates",
-				"CheckVolumeBinding",
 				"TestServiceAffinity",
 				"TestLabelsPresence",
 			),
@@ -782,6 +781,7 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 			),
 			wantFilterPlugins: sets.NewString(
 				"TaintToleration",
+				"VolumeBinder",
 			),
 			wantExtenders: []schedulerapi.ExtenderConfig{{
 				URLPrefix:        "/prefix",
@@ -877,7 +877,6 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 				"MaxCSIVolumeCountPred",
 				"MatchInterPodAffinity",
 				"GeneralPredicates",
-				"CheckVolumeBinding",
 				"TestServiceAffinity",
 				"TestLabelsPresence",
 			),
@@ -896,6 +895,7 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 			),
 			wantFilterPlugins: sets.NewString(
 				"TaintToleration",
+				"VolumeBinder",
 			),
 			wantExtenders: []schedulerapi.ExtenderConfig{{
 				URLPrefix:        "/prefix",
@@ -991,7 +991,6 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 				"MaxCinderVolumeCount",
 				"MatchInterPodAffinity",
 				"GeneralPredicates",
-				"CheckVolumeBinding",
 				"TestServiceAffinity",
 				"TestLabelsPresence",
 			),
@@ -1010,6 +1009,7 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 			),
 			wantFilterPlugins: sets.NewString(
 				"TaintToleration",
+				"VolumeBinder",
 			),
 			wantExtenders: []schedulerapi.ExtenderConfig{{
 				URLPrefix:        "/prefix",
@@ -1109,7 +1109,6 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 				"MaxCinderVolumeCount",
 				"MatchInterPodAffinity",
 				"GeneralPredicates",
-				"CheckVolumeBinding",
 				"TestServiceAffinity",
 				"TestLabelsPresence",
 			),
@@ -1128,6 +1127,7 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 			),
 			wantFilterPlugins: sets.NewString(
 				"TaintToleration",
+				"VolumeBinder",
 			),
 			wantExtenders: []schedulerapi.ExtenderConfig{{
 				URLPrefix:        "/prefix",
@@ -1151,6 +1151,7 @@ func TestCompatibility_v1_Scheduler(t *testing.T) {
 	mandatoryPredicates := sets.NewString("CheckNodeCondition")
 	filterToPredicateMap := map[string]string{
 		"TaintToleration": "PodToleratesNodeTaints",
+		"VolumeBinder":    "CheckVolumeBinding",
 	}
 
 	for v, tc := range schedulerFiles {
diff --git a/pkg/scheduler/core/extender_test.go b/pkg/scheduler/core/extender_test.go
index b7f8f78..b993543 100644
--- a/pkg/scheduler/core/extender_test.go
+++ b/pkg/scheduler/core/extender_test.go
@@ -549,7 +549,6 @@ func TestGenericSchedulerWithExtenders(t *testing.T) {
 				priorities.EmptyPriorityMetadataProducer,
 				emptyFramework,
 				extenders,
-				nil,
 				schedulertesting.FakePersistentVolumeClaimLister{},
 				schedulertesting.FakePDBLister{},
 				false,
diff --git a/pkg/scheduler/core/generic_scheduler.go b/pkg/scheduler/core/generic_scheduler.go
index 1dd238f..62b3db0 100644
--- a/pkg/scheduler/core/generic_scheduler.go
+++ b/pkg/scheduler/core/generic_scheduler.go
@@ -49,7 +49,6 @@ import (
 	"k8s.io/kubernetes/pkg/scheduler/metrics"
 	schedulernodeinfo "k8s.io/kubernetes/pkg/scheduler/nodeinfo"
 	"k8s.io/kubernetes/pkg/scheduler/util"
-	"k8s.io/kubernetes/pkg/scheduler/volumebinder"
 	utiltrace "k8s.io/utils/trace"
 )
 
@@ -153,7 +152,6 @@ type genericScheduler struct {
 	extenders                []algorithm.SchedulerExtender
 	alwaysCheckAllPredicates bool
 	nodeInfoSnapshot         *schedulernodeinfo.Snapshot
-	volumeBinder             *volumebinder.VolumeBinder
 	pvcLister                corelisters.PersistentVolumeClaimLister
 	pdbLister                algorithm.PDBLister
 	disablePreemption        bool
@@ -1282,7 +1280,6 @@ func NewGenericScheduler(
 	priorityMetaProducer priorities.PriorityMetadataProducer,
 	framework framework.Framework,
 	extenders []algorithm.SchedulerExtender,
-	volumeBinder *volumebinder.VolumeBinder,
 	pvcLister corelisters.PersistentVolumeClaimLister,
 	pdbLister algorithm.PDBLister,
 	alwaysCheckAllPredicates bool,
@@ -1300,7 +1297,6 @@ func NewGenericScheduler(
 		framework:                framework,
 		extenders:                extenders,
 		nodeInfoSnapshot:         framework.NodeInfoSnapshot(),
-		volumeBinder:             volumeBinder,
 		pvcLister:                pvcLister,
 		pdbLister:                pdbLister,
 		alwaysCheckAllPredicates: alwaysCheckAllPredicates,
diff --git a/pkg/scheduler/core/generic_scheduler_test.go b/pkg/scheduler/core/generic_scheduler_test.go
index a9157f4..e900c5a 100644
--- a/pkg/scheduler/core/generic_scheduler_test.go
+++ b/pkg/scheduler/core/generic_scheduler_test.go
@@ -667,7 +667,6 @@ func TestGenericScheduler(t *testing.T) {
 				priorities.EmptyPriorityMetadataProducer,
 				filterFramework,
 				[]algorithm.SchedulerExtender{},
-				nil,
 				pvcLister,
 				schedulertesting.FakePDBLister{},
 				test.alwaysCheckAllPredicates,
@@ -706,7 +705,7 @@ func makeScheduler(predicates map[string]algorithmpredicates.FitPredicate, nodes
 		prioritizers,
 		priorities.EmptyPriorityMetadataProducer,
 		emptyFramework,
-		nil, nil, nil, nil, false, false,
+		nil, nil, nil, false, false,
 		schedulerapi.DefaultPercentageOfNodesToScore, false)
 	cache.UpdateNodeInfoSnapshot(s.(*genericScheduler).nodeInfoSnapshot)
 	return s.(*genericScheduler)
@@ -825,7 +824,7 @@ func TestFindFitPredicateCallCounts(t *testing.T) {
 			prioritizers,
 			priorities.EmptyPriorityMetadataProducer,
 			emptyFramework,
-			nil, nil, nil, nil, false, false,
+			nil, nil, nil, false, false,
 			schedulerapi.DefaultPercentageOfNodesToScore, false).(*genericScheduler)
 		cache.UpdateNodeInfoSnapshot(scheduler.nodeInfoSnapshot)
 		queue.UpdateNominatedPodForNode(&v1.Pod{ObjectMeta: metav1.ObjectMeta{UID: types.UID("nominated")}, Spec: v1.PodSpec{Priority: &midPriority}}, "1")
@@ -1395,7 +1394,6 @@ func TestSelectNodesForPreemption(t *testing.T) {
 				filterFramework,
 				[]algorithm.SchedulerExtender{},
 				nil,
-				nil,
 				schedulertesting.FakePDBLister{},
 				false,
 				false,
@@ -2128,7 +2126,6 @@ func TestPreempt(t *testing.T) {
 				priorities.EmptyPriorityMetadataProducer,
 				emptyFramework,
 				extenders,
-				nil,
 				schedulertesting.FakePersistentVolumeClaimLister{},
 				schedulertesting.FakePDBLister{},
 				false,
diff --git a/pkg/scheduler/eventhandlers.go b/pkg/scheduler/eventhandlers.go
index 53462e2..320e7ee 100644
--- a/pkg/scheduler/eventhandlers.go
+++ b/pkg/scheduler/eventhandlers.go
@@ -247,10 +247,6 @@ func (sched *Scheduler) deletePodFromSchedulingQueue(obj interface{}) {
 	if err := sched.SchedulingQueue.Delete(pod); err != nil {
 		utilruntime.HandleError(fmt.Errorf("unable to dequeue %T: %v", obj, err))
 	}
-	if sched.VolumeBinder != nil {
-		// Volume binder only wants to keep unassigned pods
-		sched.VolumeBinder.DeletePodBindings(pod)
-	}
 }
 
 func (sched *Scheduler) addPodToCache(obj interface{}) {
diff --git a/pkg/scheduler/factory/factory.go b/pkg/scheduler/factory/factory.go
index 735a488..dc5e133 100644
--- a/pkg/scheduler/factory/factory.go
+++ b/pkg/scheduler/factory/factory.go
@@ -59,7 +59,6 @@ import (
 	internalcache "k8s.io/kubernetes/pkg/scheduler/internal/cache"
 	cachedebugger "k8s.io/kubernetes/pkg/scheduler/internal/cache/debugger"
 	internalqueue "k8s.io/kubernetes/pkg/scheduler/internal/queue"
-	"k8s.io/kubernetes/pkg/scheduler/volumebinder"
 )
 
 const (
@@ -105,9 +104,6 @@ type Config struct {
 	// Close this to shut down the scheduler.
 	StopEverything <-chan struct{}
 
-	// VolumeBinder handles PVC/PV binding for the pod.
-	VolumeBinder *volumebinder.VolumeBinder
-
 	// Disable pod preemption or not.
 	DisablePreemption bool
 
@@ -170,9 +166,6 @@ type Configurator struct {
 	// HardPodAffinitySymmetricWeight represents the weight of implicit PreferredDuringScheduling affinity rule, in the range 0-100.
 	hardPodAffinitySymmetricWeight int32
 
-	// Handles volume binding decisions
-	volumeBinder *volumebinder.VolumeBinder
-
 	// Always check all predicates even if the middle of one predicate fails.
 	alwaysCheckAllPredicates bool
 
@@ -265,8 +258,6 @@ func NewConfigFactory(args *ConfigFactoryArgs) *Configurator {
 		pluginConfig:                   args.PluginConfig,
 		pluginConfigProducerRegistry:   args.PluginConfigProducerRegistry,
 	}
-	// Setup volume binder
-	c.volumeBinder = volumebinder.NewVolumeBinder(args.Client, args.NodeInformer, args.PvcInformer, args.PvInformer, args.StorageClassInformer, time.Duration(args.BindTimeoutSeconds)*time.Second)
 	c.scheduledPodsHasSynced = args.PodInformer.Informer().HasSynced
 
 	return c
@@ -445,7 +436,6 @@ func (c *Configurator) CreateFromKeys(predicateKeys, priorityKeys sets.String, e
 		priorityMetaProducer,
 		framework,
 		extenders,
-		c.volumeBinder,
 		c.pVCLister,
 		c.pdbLister,
 		c.alwaysCheckAllPredicates,
@@ -466,7 +456,6 @@ func (c *Configurator) CreateFromKeys(predicateKeys, priorityKeys sets.String, e
 		NextPod:         internalqueue.MakeNextPodFunc(podQueue),
 		Error:           MakeDefaultErrorFunc(c.client, podQueue, c.schedulerCache, c.StopEverything),
 		StopEverything:  c.StopEverything,
-		VolumeBinder:    c.volumeBinder,
 		SchedulingQueue: podQueue,
 		Plugins:         plugins,
 		PluginConfig:    pluginConfig,
@@ -562,6 +551,9 @@ func (c *Configurator) getPredicateConfigs(predicateKeys sets.String) (map[strin
 		}
 	}
 
+	args := *configProducerArgs
+	args.BindTimeoutSeconds = c.bindTimeoutSeconds
+
 	// Second, create the framework plugin configurations, and place them in the order
 	// that the corresponding predicates were supposed to run.
 	var plugins config.Plugins
@@ -569,7 +561,7 @@ func (c *Configurator) getPredicateConfigs(predicateKeys sets.String) (map[strin
 	for _, predicateKey := range predicates.Ordering() {
 		if asPlugins.Has(predicateKey) {
 			producer := frameworkConfigProducers[predicateKey]
-			p, pc := producer(*configProducerArgs)
+			p, pc := producer(args)
 			plugins.Append(&p)
 			pluginConfig = append(pluginConfig, pc...)
 			asPlugins.Delete(predicateKey)
@@ -579,7 +571,7 @@ func (c *Configurator) getPredicateConfigs(predicateKeys sets.String) (map[strin
 	// Third, add the rest in no specific order.
 	for predicateKey := range asPlugins {
 		producer := frameworkConfigProducers[predicateKey]
-		p, pc := producer(*configProducerArgs)
+		p, pc := producer(args)
 		plugins.Append(&p)
 		pluginConfig = append(pluginConfig, pc...)
 	}
@@ -601,7 +593,6 @@ func (c *Configurator) getAlgorithmArgs() (*PluginFactoryArgs, *plugins.ConfigPr
 		PVInfo:                         &predicates.CachedPersistentVolumeInfo{PersistentVolumeLister: c.pVLister},
 		PVCInfo:                        &predicates.CachedPersistentVolumeClaimInfo{PersistentVolumeClaimLister: c.pVCLister},
 		StorageClassInfo:               &predicates.CachedStorageClassInfo{StorageClassLister: c.storageClassLister},
-		VolumeBinder:                   c.volumeBinder,
 		HardPodAffinitySymmetricWeight: c.hardPodAffinitySymmetricWeight,
 	}, &plugins.ConfigProducerArgs{}
 }
diff --git a/pkg/scheduler/factory/plugins.go b/pkg/scheduler/factory/plugins.go
index ad25a5b..514d11f 100644
--- a/pkg/scheduler/factory/plugins.go
+++ b/pkg/scheduler/factory/plugins.go
@@ -29,7 +29,6 @@ import (
 	"k8s.io/kubernetes/pkg/scheduler/algorithm/priorities"
 	schedulerapi "k8s.io/kubernetes/pkg/scheduler/api"
 	framework "k8s.io/kubernetes/pkg/scheduler/framework/v1alpha1"
-	"k8s.io/kubernetes/pkg/scheduler/volumebinder"
 
 	"k8s.io/klog"
 )
@@ -48,7 +47,6 @@ type PluginFactoryArgs struct {
 	PVInfo                         predicates.PersistentVolumeInfo
 	PVCInfo                        predicates.PersistentVolumeClaimInfo
 	StorageClassInfo               predicates.StorageClassInfo
-	VolumeBinder                   *volumebinder.VolumeBinder
 	HardPodAffinitySymmetricWeight int32
 }
 
diff --git a/pkg/scheduler/framework/plugins/default_registry.go b/pkg/scheduler/framework/plugins/default_registry.go
index 2503da1..11b9c70 100644
--- a/pkg/scheduler/framework/plugins/default_registry.go
+++ b/pkg/scheduler/framework/plugins/default_registry.go
@@ -22,6 +22,7 @@ import (
 	"k8s.io/kubernetes/pkg/scheduler/algorithm/predicates"
 	"k8s.io/kubernetes/pkg/scheduler/apis/config"
 	"k8s.io/kubernetes/pkg/scheduler/framework/plugins/tainttoleration"
+	"k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumebinder"
 	framework "k8s.io/kubernetes/pkg/scheduler/framework/v1alpha1"
 )
 
@@ -31,6 +32,7 @@ import (
 func NewDefaultRegistry() framework.Registry {
 	return framework.Registry{
 		tainttoleration.Name: tainttoleration.New,
+		volumebinder.Name:    volumebinder.New,
 	}
 }
 
@@ -40,6 +42,8 @@ func NewDefaultRegistry() framework.Registry {
 type ConfigProducerArgs struct {
 	// Weight used for priority functions.
 	Weight int32
+	// Bind timeout seconds used for Volume Binder plugin.
+	BindTimeoutSeconds int64
 }
 
 // ConfigProducer produces a framework's configuration.
@@ -63,6 +67,17 @@ func NewDefaultConfigProducerRegistry() *ConfigProducerRegistry {
 			plugins.Filter = appendToPluginSet(plugins.Filter, tainttoleration.Name, nil)
 			return
 		})
+	registry.RegisterPredicate(predicates.CheckVolumeBindingPred,
+		func(args ConfigProducerArgs) (plugins config.Plugins, pluginConfig []config.PluginConfig) {
+			plugins.Filter = appendToPluginSet(plugins.Filter, volumebinder.Name, nil)
+			pluginConfig = append(pluginConfig, volumebinder.NewPluginConfig(args.BindTimeoutSeconds))
+			// We also needs to register following extension points.
+			plugins.Reserve = appendToPluginSet(plugins.Reserve, volumebinder.Name, nil)
+			plugins.PreBind = appendToPluginSet(plugins.PreBind, volumebinder.Name, nil)
+			plugins.Unreserve = appendToPluginSet(plugins.Unreserve, volumebinder.Name, nil)
+			plugins.PostBind = appendToPluginSet(plugins.PostBind, volumebinder.Name, nil)
+			return
+		})
 
 	return registry
 }
diff --git a/pkg/scheduler/framework/plugins/volumebinder/volumebinder.go b/pkg/scheduler/framework/plugins/volumebinder/volumebinder.go
new file mode 100644
index 0000000..48a2f89
--- /dev/null
+++ b/pkg/scheduler/framework/plugins/volumebinder/volumebinder.go
@@ -0,0 +1,203 @@
+/*
+Copyright 2019 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package volumebinder
+
+import (
+	"encoding/json"
+	"fmt"
+	"time"
+
+	v1 "k8s.io/api/core/v1"
+	"k8s.io/apimachinery/pkg/runtime"
+	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
+	"k8s.io/client-go/tools/cache"
+	"k8s.io/klog"
+	"k8s.io/kubernetes/pkg/scheduler/algorithm/predicates"
+	"k8s.io/kubernetes/pkg/scheduler/apis/config"
+	"k8s.io/kubernetes/pkg/scheduler/framework/plugins/migration"
+	framework "k8s.io/kubernetes/pkg/scheduler/framework/v1alpha1"
+	schedulernodeinfo "k8s.io/kubernetes/pkg/scheduler/nodeinfo"
+	"k8s.io/kubernetes/pkg/scheduler/volumebinder"
+)
+
+const (
+	allBoundStateKey framework.StateKey = "volume-binder-all-bound"
+)
+
+type allBoundStateData bool
+
+func (d allBoundStateData) Clone() framework.StateData {
+	return d
+}
+
+// VolumeBinderPlugin implements a scheduler plugin to bind volumes in scheduling.
+type VolumeBinderPlugin struct {
+	volumeBinder *volumebinder.VolumeBinder
+	predicate    predicates.FitPredicate
+}
+
+var _ framework.FilterPlugin = &VolumeBinderPlugin{}
+var _ framework.ReservePlugin = &VolumeBinderPlugin{}
+var _ framework.PreBindPlugin = &VolumeBinderPlugin{}
+var _ framework.UnreservePlugin = &VolumeBinderPlugin{}
+var _ framework.PostBindPlugin = &VolumeBinderPlugin{}
+
+// Name is the name of the plugin used in Registry and configurations.
+const Name = "VolumeBinder"
+
+// Name returns name of the plugin. It is used in logs, etc.
+func (plugin *VolumeBinderPlugin) Name() string {
+	return Name
+}
+
+// Filter is called by the scheduling framework.
+func (plugin *VolumeBinderPlugin) Filter(cs *framework.CycleState, pod *v1.Pod, nodeInfo *schedulernodeinfo.NodeInfo) *framework.Status {
+	if plugin.predicate == nil {
+		plugin.predicate = predicates.NewVolumeBindingPredicate(plugin.volumeBinder)
+	}
+	_, reasons, err := plugin.predicate(pod, nil, nodeInfo)
+	return migration.PredicateResultToFrameworkStatus(reasons, err)
+}
+
+// Reserve is called by the scheduling framework when the scheduler cache is
+// updated.
+func (plugin *VolumeBinderPlugin) Reserve(cs *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status {
+	allBound, err := plugin.volumeBinder.Binder.AssumePodVolumes(pod, nodeName)
+	if err != nil {
+		return migration.ErrorToFrameworkStatus(err)
+	}
+	cs.Write(allBoundStateKey, allBoundStateData(allBound))
+	return nil
+}
+
+// PreBind is called before binding a pod. All prebind plugins must return
+// success or the pod will be rejected and won't be sent for binding.
+//
+// PreBind will make the API update with the assumed bindings and wait until
+// the PV controller has completely finished the binding operation.
+//
+// If binding errors, times out or gets undone, then an error will be returned to
+// retry scheduling.
+func (plugin *VolumeBinderPlugin) PreBind(cs *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status {
+	state, err := cs.Read(allBoundStateKey)
+	if err != nil {
+		return migration.ErrorToFrameworkStatus(err)
+	}
+	allBound, ok := state.(allBoundStateData)
+	if !ok {
+		return migration.ErrorToFrameworkStatus(fmt.Errorf("unable to convert state into allBoundStateData"))
+	}
+	if allBound {
+		// no need to bind volumes
+		return nil
+	}
+	klog.V(5).Infof("Trying to bind volumes for pod \"%v/%v\"", pod.Namespace, pod.Name)
+	err = plugin.volumeBinder.Binder.BindPodVolumes(pod)
+	if err != nil {
+		klog.V(1).Infof("Failed to bind volumes for pod \"%v/%v\": %v", pod.Namespace, pod.Name, err)
+		return migration.ErrorToFrameworkStatus(err)
+	}
+	klog.V(5).Infof("Success binding volumes for pod \"%v/%v\"", pod.Namespace, pod.Name)
+	return nil
+}
+
+// Unreserve is called by the scheduling framework when a reserved pod was
+// rejected in a later phase.
+// TODO(cofyc) Revert assumed PV/PVC cache here, see http://issues.k8s.io/82934#issuecomment-538269188
+func (plugin *VolumeBinderPlugin) Unreserve(cs *framework.CycleState, pod *v1.Pod, nodeName string) {
+	plugin.volumeBinder.DeletePodBindings(pod)
+	return
+}
+
+// PostBind is called after a pod is successfully bound.
+func (plugin *VolumeBinderPlugin) PostBind(cs *framework.CycleState, pod *v1.Pod, nodeName string) {
+	plugin.volumeBinder.DeletePodBindings(pod)
+	return
+}
+
+// Args represents volume binder arguments.
+type Args struct {
+	BindTimeoutSeconds *int64 `json:"bindTimeoutSeconds,omitempty"`
+}
+
+// New initializes a new plugin and returns it.
+func New(config *runtime.Unknown, handle framework.FrameworkHandle) (framework.Plugin, error) {
+	nodeInformer := handle.SharedInformerFactory().Core().V1().Nodes()
+	pvcInformer := handle.SharedInformerFactory().Core().V1().PersistentVolumeClaims()
+	pvInformer := handle.SharedInformerFactory().Core().V1().PersistentVolumes()
+	storageClassInformer := handle.SharedInformerFactory().Storage().V1().StorageClasses()
+	var args Args
+	if err := framework.DecodeInto(config, &args); err != nil {
+		return nil, err
+	}
+	// args.BindTimeoutSeconds is derived from schedulerOptions.bindTimeoutSeconds which has default value set
+	if args.BindTimeoutSeconds == nil {
+		return nil, fmt.Errorf("failed to initialize %s plugin, expects bindTimeoutSeconds not to be nil", Name)
+	}
+	volumeBinder := volumebinder.NewVolumeBinder(handle.ClientSet(), nodeInformer, pvcInformer, pvInformer, storageClassInformer, time.Duration(*args.BindTimeoutSeconds)*time.Second)
+	// In Filter phrase, pod binding cache is created for the pod and used in
+	// Reserve and PreBind phrases. Pod binding cache will be cleared at
+	// Unreserve and PostBind extension points.  However, if pod fails before
+	// Reserve phras and is deleted from the apiserver later, its pod binding
+	// cache cannot be cleared at plugin extension points. Here we register an
+	// event handler to clear pod binding cache when the pod is deleted to
+	// prevent memory leaking.
+	// TODO(cofyc) Because pod binding cache is used only in current scheduling
+	// cycle, we can share it via framework.CycleState. Then we don't need to
+	// clear pod binding cache.
+	handle.SharedInformerFactory().Core().V1().Pods().Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
+		DeleteFunc: func(obj interface{}) {
+			var pod *v1.Pod
+			switch t := obj.(type) {
+			case *v1.Pod:
+				pod = obj.(*v1.Pod)
+			case cache.DeletedFinalStateUnknown:
+				var ok bool
+				pod, ok = t.Obj.(*v1.Pod)
+				if !ok {
+					utilruntime.HandleError(fmt.Errorf("unable to convert object %T to *v1.Pod", obj))
+					return
+				}
+			default:
+				utilruntime.HandleError(fmt.Errorf("unable to handle object %T", obj))
+				return
+			}
+			volumeBinder.DeletePodBindings(pod)
+		},
+	})
+	return &VolumeBinderPlugin{
+		volumeBinder: volumeBinder,
+	}, nil
+}
+
+// NewPluginConfig initializes a PluginConfig.
+func NewPluginConfig(bindTimeoutSeconds int64) config.PluginConfig {
+	pluginArgs := Args{}
+	pluginArgs.BindTimeoutSeconds = new(int64)
+	*pluginArgs.BindTimeoutSeconds = bindTimeoutSeconds
+	rawBytes, err := json.Marshal(pluginArgs)
+	if err != nil {
+		panic("unreachable")
+	}
+	return config.PluginConfig{
+		Name: Name,
+		Args: runtime.Unknown{
+			ContentType: runtime.ContentTypeJSON,
+			Raw:         rawBytes,
+		},
+	}
+}
diff --git a/pkg/scheduler/framework/plugins/volumebinder/volumebinder_test.go b/pkg/scheduler/framework/plugins/volumebinder/volumebinder_test.go
new file mode 100644
index 0000000..cb2243b
--- /dev/null
+++ b/pkg/scheduler/framework/plugins/volumebinder/volumebinder_test.go
@@ -0,0 +1,139 @@
+/*
+Copyright 2019 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package volumebinder
+
+import (
+	"testing"
+
+	"github.com/google/go-cmp/cmp"
+	"k8s.io/apimachinery/pkg/runtime"
+	"k8s.io/client-go/informers"
+	"k8s.io/client-go/kubernetes/fake"
+	"k8s.io/kubernetes/pkg/scheduler/apis/config"
+	framework "k8s.io/kubernetes/pkg/scheduler/framework/v1alpha1"
+)
+
+var registry framework.Registry = func() framework.Registry {
+	r := make(framework.Registry)
+	r.Register(Name, New)
+	return r
+}()
+
+func TestNew(t *testing.T) {
+	tests := []struct {
+		name    string
+		args    []config.PluginConfig
+		initErr bool
+	}{
+		{
+			name:    "no arg",
+			initErr: true,
+		},
+		{
+			name: "has arg but no bindTimeoutSeconds",
+			args: []config.PluginConfig{
+				{
+					Name: Name,
+					Args: runtime.Unknown{
+						ContentType: runtime.ContentTypeJSON,
+						Raw:         []byte(`{}`),
+					},
+				},
+			},
+			initErr: true,
+		},
+		{
+			name: "has arg and bindTimeoutSeconds in json format",
+			args: []config.PluginConfig{
+				{
+					Name: Name,
+					Args: runtime.Unknown{
+						ContentType: runtime.ContentTypeJSON,
+						Raw: []byte(`{
+"bindTimeoutSeconds": 200
+}`),
+					},
+				},
+			},
+			initErr: false,
+		},
+		{
+			name: "has arg and bindTimeoutSeconds in yaml format",
+			args: []config.PluginConfig{
+				{
+					Name: Name,
+					Args: runtime.Unknown{
+						ContentType: runtime.ContentTypeYAML,
+						Raw: []byte(`
+bindTimeoutSeconds: 200
+`),
+					},
+				},
+			},
+			initErr: false,
+		},
+	}
+
+	plugins := &config.Plugins{
+		Filter: &config.PluginSet{
+			Enabled: []config.Plugin{
+				{
+					Name: Name,
+				},
+			},
+		},
+	}
+
+	clientset := fake.NewSimpleClientset()
+	sharedInformerFactory := informers.NewSharedInformerFactory(clientset, 0)
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			_, err := framework.NewFramework(registry, plugins, tt.args, framework.WithClientSet(clientset), framework.WithInformerFactory(sharedInformerFactory))
+			if tt.initErr && err == nil {
+				t.Fatal("Framework initialization should fail")
+			}
+			if !tt.initErr && err != nil {
+				t.Fatalf("Failed to create framework for testing: %v", err)
+			}
+		})
+	}
+}
+
+func TestNewPluginConfig(t *testing.T) {
+	tests := []struct {
+		bindTimeoutSeconds int64
+		expected           config.PluginConfig
+	}{
+		{
+			bindTimeoutSeconds: 200,
+			expected: config.PluginConfig{
+				Name: Name,
+				Args: runtime.Unknown{
+					ContentType: runtime.ContentTypeJSON,
+					Raw:         []byte(`{"bindTimeoutSeconds":200}`),
+				},
+			},
+		},
+	}
+
+	for _, tt := range tests {
+		config := NewPluginConfig(200)
+		if diff := cmp.Diff(tt.expected, config); diff != "" {
+			t.Errorf("unexpected plugin configuration (-want, +got): %s", diff)
+		}
+	}
+}
diff --git a/pkg/scheduler/scheduler.go b/pkg/scheduler/scheduler.go
index 92b0874..708bdc2 100644
--- a/pkg/scheduler/scheduler.go
+++ b/pkg/scheduler/scheduler.go
@@ -47,7 +47,6 @@ import (
 	internalcache "k8s.io/kubernetes/pkg/scheduler/internal/cache"
 	internalqueue "k8s.io/kubernetes/pkg/scheduler/internal/queue"
 	"k8s.io/kubernetes/pkg/scheduler/metrics"
-	"k8s.io/kubernetes/pkg/scheduler/volumebinder"
 )
 
 const (
@@ -102,9 +101,6 @@ type Scheduler struct {
 	// Close this to shut down the scheduler.
 	StopEverything <-chan struct{}
 
-	// VolumeBinder handles PVC/PV binding for the pod.
-	VolumeBinder *volumebinder.VolumeBinder
-
 	// Disable pod preemption or not.
 	DisablePreemption bool
 
@@ -354,7 +350,6 @@ func NewFromConfig(config *factory.Config) *Scheduler {
 		Error:             config.Error,
 		Recorder:          config.Recorder,
 		StopEverything:    config.StopEverything,
-		VolumeBinder:      config.VolumeBinder,
 		DisablePreemption: config.DisablePreemption,
 		SchedulingQueue:   config.SchedulingQueue,
 	}
@@ -457,42 +452,6 @@ func (sched *Scheduler) preempt(state *framework.CycleState, fwk framework.Frame
 	return nodeName, err
 }
 
-// assumeVolumes will update the volume cache with the chosen bindings
-//
-// This function modifies assumed if volume binding is required.
-func (sched *Scheduler) assumeVolumes(assumed *v1.Pod, host string) (allBound bool, err error) {
-	allBound, err = sched.VolumeBinder.Binder.AssumePodVolumes(assumed, host)
-	if err != nil {
-		sched.recordSchedulingFailure(assumed, err, SchedulerError,
-			fmt.Sprintf("AssumePodVolumes failed: %v", err))
-	}
-	return
-}
-
-// bindVolumes will make the API update with the assumed bindings and wait until
-// the PV controller has completely finished the binding operation.
-//
-// If binding errors, times out or gets undone, then an error will be returned to
-// retry scheduling.
-func (sched *Scheduler) bindVolumes(assumed *v1.Pod) error {
-	klog.V(5).Infof("Trying to bind volumes for pod \"%v/%v\"", assumed.Namespace, assumed.Name)
-	err := sched.VolumeBinder.Binder.BindPodVolumes(assumed)
-	if err != nil {
-		klog.V(1).Infof("Failed to bind volumes for pod \"%v/%v\": %v", assumed.Namespace, assumed.Name, err)
-
-		// Unassume the Pod and retry scheduling
-		if forgetErr := sched.SchedulerCache.ForgetPod(assumed); forgetErr != nil {
-			klog.Errorf("scheduler cache ForgetPod failed: %v", forgetErr)
-		}
-
-		sched.recordSchedulingFailure(assumed, err, "VolumeBindingFailed", err.Error())
-		return err
-	}
-
-	klog.V(5).Infof("Success binding volumes for pod \"%v/%v\"", assumed.Namespace, assumed.Name)
-	return nil
-}
-
 // assume signals to the cache that a pod is already in the cache, so that binding can be asynchronous.
 // assume modifies `assumed`.
 func (sched *Scheduler) assume(assumed *v1.Pod, host string) error {
@@ -618,20 +577,6 @@ func (sched *Scheduler) scheduleOne() {
 	// This allows us to keep scheduling without waiting on binding to occur.
 	assumedPod := pod.DeepCopy()
 
-	// Assume volumes first before assuming the pod.
-	//
-	// If all volumes are completely bound, then allBound is true and binding will be skipped.
-	//
-	// Otherwise, binding of volumes is started after the pod is assumed, but before pod binding.
-	//
-	// This function modifies 'assumedPod' if volume binding is required.
-	allBound, err := sched.assumeVolumes(assumedPod, scheduleResult.SuggestedHost)
-	if err != nil {
-		klog.Errorf("error assuming volumes: %v", err)
-		metrics.PodScheduleErrors.Inc()
-		return
-	}
-
 	// Run "reserve" plugins.
 	if sts := fwk.RunReservePlugins(state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {
 		sched.recordSchedulingFailure(assumedPod, sts.AsError(), SchedulerError, sts.Message())
@@ -650,18 +595,6 @@ func (sched *Scheduler) scheduleOne() {
 	}
 	// bind the pod to its host asynchronously (we can do this b/c of the assumption step above).
 	go func() {
-		// Bind volumes first before Pod
-		if !allBound {
-			err := sched.bindVolumes(assumedPod)
-			if err != nil {
-				klog.Errorf("error binding volumes: %v", err)
-				metrics.PodScheduleErrors.Inc()
-				// trigger un-reserve plugins to clean up state associated with the reserved Pod
-				fwk.RunUnreservePlugins(state, assumedPod, scheduleResult.SuggestedHost)
-				return
-			}
-		}
-
 		// Run "permit" plugins.
 		permitStatus := fwk.RunPermitPlugins(state, assumedPod, scheduleResult.SuggestedHost)
 		if !permitStatus.IsSuccess() {
diff --git a/pkg/scheduler/scheduler_test.go b/pkg/scheduler/scheduler_test.go
index 8490926..38c057a 100644
--- a/pkg/scheduler/scheduler_test.go
+++ b/pkg/scheduler/scheduler_test.go
@@ -41,7 +41,6 @@ import (
 	"k8s.io/client-go/kubernetes/scheme"
 	clientcache "k8s.io/client-go/tools/cache"
 	"k8s.io/client-go/tools/events"
-	volumescheduling "k8s.io/kubernetes/pkg/controller/volume/scheduling"
 	"k8s.io/kubernetes/pkg/scheduler/algorithm"
 	"k8s.io/kubernetes/pkg/scheduler/algorithm/predicates"
 	"k8s.io/kubernetes/pkg/scheduler/algorithm/priorities"
@@ -54,7 +53,6 @@ import (
 	fakecache "k8s.io/kubernetes/pkg/scheduler/internal/cache/fake"
 	internalqueue "k8s.io/kubernetes/pkg/scheduler/internal/queue"
 	schedulernodeinfo "k8s.io/kubernetes/pkg/scheduler/nodeinfo"
-	"k8s.io/kubernetes/pkg/scheduler/volumebinder"
 )
 
 var (
@@ -298,9 +296,8 @@ func TestScheduler(t *testing.T) {
 				NextPod: func() *v1.Pod {
 					return item.sendPod
 				},
-				Framework:    emptyFramework,
-				Recorder:     eventBroadcaster.NewRecorder(scheme.Scheme, "scheduler"),
-				VolumeBinder: volumebinder.NewFakeVolumeBinder(&volumescheduling.FakeVolumeBinderConfig{AllBound: true}),
+				Framework: emptyFramework,
+				Recorder:  eventBroadcaster.NewRecorder(scheme.Scheme, "scheduler"),
 			}
 			called := make(chan struct{})
 			stopFunc := eventBroadcaster.StartEventWatcher(func(obj runtime.Object) {
@@ -654,7 +651,6 @@ func setupTestScheduler(queuedPodStore *clientcache.FIFO, scache internalcache.C
 		priorities.EmptyPriorityMetadataProducer,
 		emptyFramework,
 		[]algorithm.SchedulerExtender{},
-		nil,
 		informerFactory.Core().V1().PersistentVolumeClaims().Lister(),
 		informerFactory.Policy().V1beta1().PodDisruptionBudgets().Lister(),
 		false,
@@ -684,7 +680,6 @@ func setupTestScheduler(queuedPodStore *clientcache.FIFO, scache internalcache.C
 		podConditionUpdater: fakePodConditionUpdater{},
 		PodPreemptor:        fakePodPreemptor{},
 		Framework:           emptyFramework,
-		VolumeBinder:        volumebinder.NewFakeVolumeBinder(&volumescheduling.FakeVolumeBinderConfig{AllBound: true}),
 	}
 
 	if recorder != nil {
@@ -705,7 +700,6 @@ func setupTestSchedulerLongBindingWithRetry(queuedPodStore *clientcache.FIFO, sc
 		priorities.EmptyPriorityMetadataProducer,
 		framework,
 		[]algorithm.SchedulerExtender{},
-		nil,
 		informerFactory.Core().V1().PersistentVolumeClaims().Lister(),
 		informerFactory.Policy().V1beta1().PodDisruptionBudgets().Lister(),
 		false,
@@ -739,38 +733,11 @@ func setupTestSchedulerLongBindingWithRetry(queuedPodStore *clientcache.FIFO, sc
 		PodPreemptor:        fakePodPreemptor{},
 		StopEverything:      stop,
 		Framework:           framework,
-		VolumeBinder:        volumebinder.NewFakeVolumeBinder(&volumescheduling.FakeVolumeBinderConfig{AllBound: true}),
 	}
 
 	return sched, bindingChan
 }
 
-func setupTestSchedulerWithVolumeBinding(fakeVolumeBinder *volumebinder.VolumeBinder, stop <-chan struct{}, broadcaster events.EventBroadcaster) (*Scheduler, chan *v1.Binding, chan error) {
-	testNode := v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "machine1", UID: types.UID("machine1")}}
-	queuedPodStore := clientcache.NewFIFO(clientcache.MetaNamespaceKeyFunc)
-	pod := podWithID("foo", "")
-	pod.Namespace = "foo-ns"
-	pod.Spec.Volumes = append(pod.Spec.Volumes, v1.Volume{Name: "testVol",
-		VolumeSource: v1.VolumeSource{PersistentVolumeClaim: &v1.PersistentVolumeClaimVolumeSource{ClaimName: "testPVC"}}})
-	queuedPodStore.Add(pod)
-	scache := internalcache.New(10*time.Minute, stop)
-	scache.AddNode(&testNode)
-	testPVC := v1.PersistentVolumeClaim{ObjectMeta: metav1.ObjectMeta{Name: "testPVC", Namespace: pod.Namespace, UID: types.UID("testPVC")}}
-	client := clientsetfake.NewSimpleClientset(&testNode, &testPVC)
-	informerFactory := informers.NewSharedInformerFactory(client, 0)
-
-	predicateMap := map[string]predicates.FitPredicate{
-		predicates.CheckVolumeBindingPred: predicates.NewVolumeBindingPredicate(fakeVolumeBinder),
-	}
-
-	recorder := broadcaster.NewRecorder(scheme.Scheme, "scheduler")
-	s, bindingChan, errChan := setupTestScheduler(queuedPodStore, scache, informerFactory, predicateMap, recorder)
-	informerFactory.Start(stop)
-	informerFactory.WaitForCacheSync(stop)
-	s.VolumeBinder = fakeVolumeBinder
-	return s, bindingChan, errChan
-}
-
 // This is a workaround because golint complains that errors cannot
 // end with punctuation.  However, the real predicate error message does
 // end with a period.
@@ -779,171 +746,6 @@ func makePredicateError(failReason string) error {
 	return fmt.Errorf(s)
 }
 
-func TestSchedulerWithVolumeBinding(t *testing.T) {
-	findErr := fmt.Errorf("find err")
-	assumeErr := fmt.Errorf("assume err")
-	bindErr := fmt.Errorf("bind err")
-	client := clientsetfake.NewSimpleClientset()
-
-	eventBroadcaster := events.NewBroadcaster(&events.EventSinkImpl{Interface: client.EventsV1beta1().Events("")})
-
-	// This can be small because we wait for pod to finish scheduling first
-	chanTimeout := 2 * time.Second
-
-	table := []struct {
-		name               string
-		expectError        error
-		expectPodBind      *v1.Binding
-		expectAssumeCalled bool
-		expectBindCalled   bool
-		eventReason        string
-		volumeBinderConfig *volumescheduling.FakeVolumeBinderConfig
-	}{
-		{
-			name: "all bound",
-			volumeBinderConfig: &volumescheduling.FakeVolumeBinderConfig{
-				AllBound:             true,
-				FindUnboundSatsified: true,
-				FindBoundSatsified:   true,
-			},
-			expectAssumeCalled: true,
-			expectPodBind:      &v1.Binding{ObjectMeta: metav1.ObjectMeta{Name: "foo", Namespace: "foo-ns", UID: types.UID("foo")}, Target: v1.ObjectReference{Kind: "Node", Name: "machine1"}},
-			eventReason:        "Scheduled",
-		},
-		{
-			name: "bound/invalid pv affinity",
-			volumeBinderConfig: &volumescheduling.FakeVolumeBinderConfig{
-				AllBound:             true,
-				FindUnboundSatsified: true,
-				FindBoundSatsified:   false,
-			},
-			eventReason: "FailedScheduling",
-			expectError: makePredicateError("1 node(s) had volume node affinity conflict"),
-		},
-		{
-			name: "unbound/no matches",
-			volumeBinderConfig: &volumescheduling.FakeVolumeBinderConfig{
-				FindUnboundSatsified: false,
-				FindBoundSatsified:   true,
-			},
-			eventReason: "FailedScheduling",
-			expectError: makePredicateError("1 node(s) didn't find available persistent volumes to bind"),
-		},
-		{
-			name: "bound and unbound unsatisfied",
-			volumeBinderConfig: &volumescheduling.FakeVolumeBinderConfig{
-				FindUnboundSatsified: false,
-				FindBoundSatsified:   false,
-			},
-			eventReason: "FailedScheduling",
-			expectError: makePredicateError("1 node(s) didn't find available persistent volumes to bind, 1 node(s) had volume node affinity conflict"),
-		},
-		{
-			name: "unbound/found matches/bind succeeds",
-			volumeBinderConfig: &volumescheduling.FakeVolumeBinderConfig{
-				FindUnboundSatsified: true,
-				FindBoundSatsified:   true,
-			},
-			expectAssumeCalled: true,
-			expectBindCalled:   true,
-			expectPodBind:      &v1.Binding{ObjectMeta: metav1.ObjectMeta{Name: "foo", Namespace: "foo-ns", UID: types.UID("foo")}, Target: v1.ObjectReference{Kind: "Node", Name: "machine1"}},
-			eventReason:        "Scheduled",
-		},
-		{
-			name: "predicate error",
-			volumeBinderConfig: &volumescheduling.FakeVolumeBinderConfig{
-				FindErr: findErr,
-			},
-			eventReason: "FailedScheduling",
-			expectError: findErr,
-		},
-		{
-			name: "assume error",
-			volumeBinderConfig: &volumescheduling.FakeVolumeBinderConfig{
-				FindUnboundSatsified: true,
-				FindBoundSatsified:   true,
-				AssumeErr:            assumeErr,
-			},
-			expectAssumeCalled: true,
-			eventReason:        "FailedScheduling",
-			expectError:        assumeErr,
-		},
-		{
-			name: "bind error",
-			volumeBinderConfig: &volumescheduling.FakeVolumeBinderConfig{
-				FindUnboundSatsified: true,
-				FindBoundSatsified:   true,
-				BindErr:              bindErr,
-			},
-			expectAssumeCalled: true,
-			expectBindCalled:   true,
-			eventReason:        "FailedScheduling",
-			expectError:        bindErr,
-		},
-	}
-
-	for _, item := range table {
-		t.Run(item.name, func(t *testing.T) {
-			stop := make(chan struct{})
-			fakeVolumeBinder := volumebinder.NewFakeVolumeBinder(item.volumeBinderConfig)
-			internalBinder, ok := fakeVolumeBinder.Binder.(*volumescheduling.FakeVolumeBinder)
-			if !ok {
-				t.Fatalf("Failed to get fake volume binder")
-			}
-			s, bindingChan, errChan := setupTestSchedulerWithVolumeBinding(fakeVolumeBinder, stop, eventBroadcaster)
-			eventChan := make(chan struct{})
-			stopFunc := eventBroadcaster.StartEventWatcher(func(obj runtime.Object) {
-				e, _ := obj.(*v1beta1.Event)
-				if e, a := item.eventReason, e.Reason; e != a {
-					t.Errorf("expected %v, got %v", e, a)
-				}
-				close(eventChan)
-			})
-			s.scheduleOne()
-			// Wait for pod to succeed or fail scheduling
-			select {
-			case <-eventChan:
-			case <-time.After(wait.ForeverTestTimeout):
-				t.Fatalf("scheduling timeout after %v", wait.ForeverTestTimeout)
-			}
-			stopFunc()
-			// Wait for scheduling to return an error
-			select {
-			case err := <-errChan:
-				if item.expectError == nil || !reflect.DeepEqual(item.expectError.Error(), err.Error()) {
-					t.Errorf("err \nWANT=%+v,\nGOT=%+v", item.expectError, err)
-				}
-			case <-time.After(chanTimeout):
-				if item.expectError != nil {
-					t.Errorf("did not receive error after %v", chanTimeout)
-				}
-			}
-
-			// Wait for pod to succeed binding
-			select {
-			case b := <-bindingChan:
-				if !reflect.DeepEqual(item.expectPodBind, b) {
-					t.Errorf("err \nWANT=%+v,\nGOT=%+v", item.expectPodBind, b)
-				}
-			case <-time.After(chanTimeout):
-				if item.expectPodBind != nil {
-					t.Errorf("did not receive pod binding after %v", chanTimeout)
-				}
-			}
-
-			if item.expectAssumeCalled != internalBinder.AssumeCalled {
-				t.Errorf("expectedAssumeCall %v", item.expectAssumeCalled)
-			}
-
-			if item.expectBindCalled != internalBinder.BindCalled {
-				t.Errorf("expectedBindCall %v", item.expectBindCalled)
-			}
-
-			close(stop)
-		})
-	}
-}
-
 func TestInitPolicyFromFile(t *testing.T) {
 	dir, err := ioutil.TempDir(os.TempDir(), "policy")
 	if err != nil {
